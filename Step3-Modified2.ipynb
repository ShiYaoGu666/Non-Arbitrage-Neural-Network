{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3-Modified2 \n",
    "\n",
    "By experiment, we find that Pytorch is much more better then Scipy while solving this problem. But unfortunately, if we want to use Pytorch instead, we need to change the format of matrix from numpy array to Pytorch Tensor. And that's why we deside to rewrite the program once again. \n",
    "\n",
    "Before you start, make sure you have the Pytorch Package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def softplus(x):\n",
    "    return F.softplus(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(V, b, w, w0, x):\n",
    "    \"\"\"\n",
    "    Perform forward propagation using PyTorch.\n",
    "    \n",
    "    Arguments:\n",
    "    V -- weight matrix for the first layer\n",
    "    b -- bias matrix for the first layer\n",
    "    w -- weight vector for the output layer\n",
    "    w0 -- bias scalar for the output layer\n",
    "    x -- input data matrix\n",
    "    \n",
    "    Returns:\n",
    "    result -- the output of the forward propagation\n",
    "    \"\"\"\n",
    "    V_first_column = V[:, 0].reshape(-1, 1)\n",
    "    V_second_column = V[:, 1].reshape(-1, 1)\n",
    "    x_first_row = x[0, :].reshape(1, -1)\n",
    "    x_second_row = x[1, :].reshape(1, -1)\n",
    "    b_first_column = b[:, 0].reshape(-1, 1)\n",
    "    b_second_column = b[:, 1].reshape(-1, 1)\n",
    "    \n",
    "    parameters_ready_to_apply_softplus = torch.mm(torch.exp(V_first_column), x_first_row) + b_first_column\n",
    "    parameters_ready_to_apply_sigmoid = torch.mm(torch.exp(V_second_column), x_second_row) + b_second_column\n",
    "    \n",
    "    after_softplus = softplus(parameters_ready_to_apply_softplus)\n",
    "    after_sigmoid = sigmoid(parameters_ready_to_apply_sigmoid)\n",
    "\n",
    "    layer1 = after_softplus * after_sigmoid\n",
    "\n",
    "    result = torch.mm(torch.exp(w).unsqueeze(0), layer1) + torch.exp(w0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the input value x is a vector, looks like:\n",
    "\n",
    "$\\left(\\begin{array}{cccc}K_1 & K_2 & & K_N \\\\ S_1 & S_2 & \\cdots & S_N\\end{array}\\right)$\n",
    "\n",
    "So, the output value should be a row vector: \n",
    "\n",
    "$\\left(\\hat{Y}_1, \\hat{Y}_2, \\hat{Y}_3 \\cdots ., \\hat{Y}_N\\right)$\n",
    "\n",
    "The following is the shape of other parameters: \n",
    "\n",
    "$V=\\left(\\begin{array}{cc}v_{11} & v_{12} \\\\ v_{21} & v_{22} \\\\ \\vdots &\\vdots \\\\ v_{H 1} & v_{H2}\\end{array}\\right) \\quad b=\\left(\\begin{array}{cc}b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ \\vdots& \\vdots \\\\ b_{H1} & b_{H2}\\end{array}\\right)$\n",
    "\n",
    "$w=\\left(w, w_2, w_3, w_4 \\cdots w_H\\right)$\n",
    "\n",
    "$w_0 \\in R $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just compute MSE: \n",
    "\n",
    "$ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (AL_i - y_i)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(AL, Y):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    AL -- predicted values tensor\n",
    "    Y -- true labels tensor\n",
    "    \n",
    "    Returns:\n",
    "    mse -- mean squared error tensor\n",
    "    \"\"\"\n",
    "    mse = torch.mean((AL - Y) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def read_option_data(file_path, ticker, current_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Reads option data from an Excel file, filters it based on the given ticker and option type 'Call',\n",
    "    calculates the days to expiration from the current_date, and returns matrices of strikes with days to expiration,\n",
    "    and stock prices.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the Excel file.\n",
    "    ticker (str): The ticker symbol to filter the data.\n",
    "    current_date (datetime): The current date for calculating days to expiration.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Two matrices, one for strikes and days to expiration, and one for stock prices.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Filter data for the specified ticker and for Call options\n",
    "    df_filtered = df[(df['Ticker'] == ticker) & (df['Type'] == 'Call')].copy()\n",
    "    \n",
    "    # Convert Expiration(T) to datetime\n",
    "    df_filtered['Expiration(T)'] = pd.to_datetime(df_filtered['Expiration(T)'])\n",
    "    \n",
    "    # Calculate the difference in days between Expiration(T) and current_date\n",
    "    df_filtered['Days to Expiration'] = (df_filtered['Expiration(T)'] - current_date).dt.days\n",
    "    \n",
    "    # Extract K, Days to Expiration and T columns to form one matrix\n",
    "    K_T_matrix = df_filtered[['Strike(K)', 'Days to Expiration']].values\n",
    "    \n",
    "    # Extract S column to form another matrix\n",
    "    c_matrix = df_filtered[['Last Option Price (c)']].values\n",
    "    \n",
    "    return K_T_matrix.T, c_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we know that we should have : \n",
    "\n",
    "$ f \\geq 0, \\quad \\frac{\\partial f}{\\partial T} \\geq 0, \\quad \\frac{\\partial f}{\\partial K} \\leq 0, \\quad \\frac{\\partial^2 f}{\\partial K^2} \\geq 0$\n",
    "\n",
    "So we take $ K $ as the first element, and take $ K $ as $-K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_first_row(matrix):\n",
    "    \"\"\"\n",
    "    Negates the elements of the first row of the given matrix.\n",
    "\n",
    "    Parameters:\n",
    "    matrix (numpy.ndarray): The input matrix whose first row elements will be negated.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The modified matrix with the first row elements negated.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Negate the first row of the matrix\n",
    "    matrix[0] = -matrix[0]\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_apple, y_apple = read_option_data('/Users/shiyaogu/Documents/Modules in UoL/MATH 391/Attempt/options_data-2.xlsx', \"AAPL\",  datetime(2024, 6, 26))\n",
    "x_tesla, y_tesla = read_option_data('/Users/shiyaogu/Documents/Modules in UoL/MATH 391/Attempt/options_data-2.xlsx', \"TSLA\",  datetime(2024, 6, 26))\n",
    "x_apple_final = torch.tensor(negate_first_row(x_apple), dtype = torch.float32)\n",
    "x_tesla_final = torch.tensor(negate_first_row(x_tesla), dtype = torch.float32)\n",
    "y_apple_final = torch.tensor(y_apple, dtype = torch.float32)\n",
    "y_tesla_final = torch.tensor(y_tesla, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def split_train_test(matrix1, matrix2, ratio):\n",
    "    \"\"\"\n",
    "    Randomly splits two given matrices into training and testing datasets based on a given ratio.\n",
    "    \n",
    "    Parameters:\n",
    "    matrix1 (torch.Tensor): The first input matrix.\n",
    "    matrix2 (torch.Tensor): The second input matrix.\n",
    "    ratio (float): The ratio of columns to include in the training data (0 < ratio < 1).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Four matrices: train_matrix1, test_matrix1, train_matrix2, test_matrix2\n",
    "    \"\"\"\n",
    "    # Ensure the input ratio is between 0 and 1\n",
    "    if not (0 < ratio < 1):\n",
    "        raise ValueError(\"The ratio must be a float between 0 and 1\")\n",
    "    \n",
    "    # Calculate the number of columns for training based on the ratio\n",
    "    n = int(matrix1.shape[1] * ratio)\n",
    "    \n",
    "    # Ensure the input matrices have enough columns\n",
    "    if matrix1.shape[1] < n or matrix2.shape[1] < n:\n",
    "        raise ValueError(\"The input matrices must have enough columns to split based on the ratio\")\n",
    "    \n",
    "    # Randomly shuffle the indices of columns\n",
    "    indices = torch.randperm(matrix1.shape[1])\n",
    "    train_indices = indices[:n]\n",
    "    test_indices = indices[n:]\n",
    "    \n",
    "    # Split the first matrix\n",
    "    train_matrix1 = matrix1[:, train_indices]\n",
    "    test_matrix1 = matrix1[:, test_indices]\n",
    "    \n",
    "    # Split the second matrix\n",
    "    train_matrix2 = matrix2[:, train_indices]\n",
    "    test_matrix2 = matrix2[:, test_indices]\n",
    "    \n",
    "    return train_matrix1, test_matrix1, train_matrix2, test_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 824])\n",
      "torch.Size([1, 824])\n"
     ]
    }
   ],
   "source": [
    "train_x_apple, test_x_apple, train_y_apple, test_y_apple = split_train_test(x_apple_final, y_apple_final, 0.9)\n",
    "train_x_tesla, test_x_tesla, train_y_tesla, test_y_tesla = split_train_test(x_tesla_final, y_tesla_final, 0.9)\n",
    "print(train_x_apple.shape)\n",
    "print(train_y_apple.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/50000], Loss: 43911.4258\n",
      "Epoch [200/50000], Loss: 11511.7500\n",
      "Epoch [300/50000], Loss: 5618.8203\n",
      "Epoch [400/50000], Loss: 3342.1614\n",
      "Epoch [500/50000], Loss: 2088.0010\n",
      "Epoch [600/50000], Loss: 1502.8826\n",
      "Epoch [700/50000], Loss: 1126.3840\n",
      "Epoch [800/50000], Loss: 873.5991\n",
      "Epoch [900/50000], Loss: 696.9634\n",
      "Epoch [1000/50000], Loss: 569.3202\n",
      "Epoch [1100/50000], Loss: 474.3331\n",
      "Epoch [1200/50000], Loss: 401.5760\n",
      "Epoch [1300/50000], Loss: 343.5269\n",
      "Epoch [1400/50000], Loss: 297.5098\n",
      "Epoch [1500/50000], Loss: 260.5342\n",
      "Epoch [1600/50000], Loss: 230.4895\n",
      "Epoch [1700/50000], Loss: 205.7788\n",
      "Epoch [1800/50000], Loss: 185.1971\n",
      "Epoch [1900/50000], Loss: 167.8491\n",
      "Epoch [2000/50000], Loss: 152.5727\n",
      "Epoch [2100/50000], Loss: 139.3475\n",
      "Epoch [2200/50000], Loss: 128.3242\n",
      "Epoch [2300/50000], Loss: 118.9269\n",
      "Epoch [2400/50000], Loss: 110.8080\n",
      "Epoch [2500/50000], Loss: 103.7824\n",
      "Epoch [2600/50000], Loss: 97.6543\n",
      "Epoch [2700/50000], Loss: 92.2795\n",
      "Epoch [2800/50000], Loss: 87.5434\n",
      "Epoch [2900/50000], Loss: 83.3582\n",
      "Epoch [3000/50000], Loss: 79.6456\n",
      "Epoch [3100/50000], Loss: 76.3397\n",
      "Epoch [3200/50000], Loss: 73.3890\n",
      "Epoch [3300/50000], Loss: 70.7477\n",
      "Epoch [3400/50000], Loss: 68.3770\n",
      "Epoch [3500/50000], Loss: 66.2456\n",
      "Epoch [3600/50000], Loss: 64.3241\n",
      "Epoch [3700/50000], Loss: 62.5892\n",
      "Epoch [3800/50000], Loss: 61.0189\n",
      "Epoch [3900/50000], Loss: 59.5971\n",
      "Epoch [4000/50000], Loss: 58.3196\n",
      "Epoch [4100/50000], Loss: 57.1334\n",
      "Epoch [4200/50000], Loss: 56.0702\n",
      "Epoch [4300/50000], Loss: 55.0941\n",
      "Epoch [4400/50000], Loss: 54.2082\n",
      "Epoch [4500/50000], Loss: 53.3987\n",
      "Epoch [4600/50000], Loss: 52.6599\n",
      "Epoch [4700/50000], Loss: 51.9839\n",
      "Epoch [4800/50000], Loss: 51.3659\n",
      "Epoch [4900/50000], Loss: 50.7992\n",
      "Epoch [5000/50000], Loss: 50.2832\n",
      "Epoch [5100/50000], Loss: 49.8058\n",
      "Epoch [5200/50000], Loss: 49.3700\n",
      "Epoch [5300/50000], Loss: 48.9706\n",
      "Epoch [5400/50000], Loss: 48.6042\n",
      "Epoch [5500/50000], Loss: 48.2691\n",
      "Epoch [5600/50000], Loss: 47.9592\n",
      "Epoch [5700/50000], Loss: 47.6760\n",
      "Epoch [5800/50000], Loss: 47.4159\n",
      "Epoch [5900/50000], Loss: 47.1773\n",
      "Epoch [6000/50000], Loss: 46.9606\n",
      "Epoch [6100/50000], Loss: 46.7569\n",
      "Epoch [6200/50000], Loss: 46.5728\n",
      "Epoch [6300/50000], Loss: 46.4030\n",
      "Epoch [6400/50000], Loss: 46.2515\n",
      "Epoch [6500/50000], Loss: 46.1051\n",
      "Epoch [6600/50000], Loss: 45.9745\n",
      "Epoch [6700/50000], Loss: 45.8548\n",
      "Epoch [6800/50000], Loss: 45.7449\n",
      "Epoch [6900/50000], Loss: 45.6443\n",
      "Epoch [7000/50000], Loss: 45.5521\n",
      "Epoch [7100/50000], Loss: 45.4679\n",
      "Epoch [7200/50000], Loss: 45.3906\n",
      "Epoch [7300/50000], Loss: 45.3200\n",
      "Epoch [7400/50000], Loss: 45.2556\n",
      "Epoch [7500/50000], Loss: 45.1961\n",
      "Epoch [7600/50000], Loss: 45.1421\n",
      "Epoch [7700/50000], Loss: 45.0919\n",
      "Epoch [7800/50000], Loss: 45.0463\n",
      "Epoch [7900/50000], Loss: 45.0046\n",
      "Epoch [8000/50000], Loss: 44.9664\n",
      "Epoch [8100/50000], Loss: 44.9314\n",
      "Epoch [8200/50000], Loss: 44.8994\n",
      "Epoch [8300/50000], Loss: 44.8700\n",
      "Epoch [8400/50000], Loss: 44.8432\n",
      "Epoch [8500/50000], Loss: 44.8182\n",
      "Epoch [8600/50000], Loss: 44.7956\n",
      "Epoch [8700/50000], Loss: 44.7745\n",
      "Epoch [8800/50000], Loss: 44.7625\n",
      "Epoch [8900/50000], Loss: 44.7371\n",
      "Epoch [9000/50000], Loss: 44.7205\n",
      "Epoch [9100/50000], Loss: 44.7049\n",
      "Epoch [9200/50000], Loss: 44.6959\n",
      "Epoch [9300/50000], Loss: 44.6768\n",
      "Epoch [9400/50000], Loss: 44.6640\n",
      "Epoch [9500/50000], Loss: 44.6520\n",
      "Epoch [9600/50000], Loss: 44.6406\n",
      "Epoch [9700/50000], Loss: 44.6298\n",
      "Epoch [9800/50000], Loss: 44.6198\n",
      "Epoch [9900/50000], Loss: 44.6095\n",
      "Epoch [10000/50000], Loss: 44.6002\n",
      "Epoch [10100/50000], Loss: 44.5908\n",
      "Epoch [10200/50000], Loss: 44.5817\n",
      "Epoch [10300/50000], Loss: 44.5070\n",
      "Epoch [10400/50000], Loss: 44.4105\n",
      "Epoch [10500/50000], Loss: 44.3891\n",
      "Epoch [10600/50000], Loss: 44.3580\n",
      "Epoch [10700/50000], Loss: 44.3373\n",
      "Epoch [10800/50000], Loss: 44.3234\n",
      "Epoch [10900/50000], Loss: 44.3107\n",
      "Epoch [11000/50000], Loss: 44.2993\n",
      "Epoch [11100/50000], Loss: 44.2876\n",
      "Epoch [11200/50000], Loss: 44.2772\n",
      "Epoch [11300/50000], Loss: 44.2667\n",
      "Epoch [11400/50000], Loss: 44.2592\n",
      "Epoch [11500/50000], Loss: 44.2468\n",
      "Epoch [11600/50000], Loss: 44.2423\n",
      "Epoch [11700/50000], Loss: 44.2268\n",
      "Epoch [11800/50000], Loss: 44.2202\n",
      "Epoch [11900/50000], Loss: 44.2069\n",
      "Epoch [12000/50000], Loss: 44.1979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m mean_squared_error(y_pred, train_y_apple)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward propagation and optimization\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Print loss every 1000 iterations\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Python3p8/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Python3p8/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Python3p8/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "H = 100000\n",
    "input_dim = 2  # Dimension of x_apple\n",
    "\n",
    "# Initialize parameters\n",
    "V = torch.randn(H, 2, requires_grad=True, dtype=torch.float32)\n",
    "b = torch.randn(H, 2, requires_grad=True, dtype=torch.float32)\n",
    "w = torch.randn(H, requires_grad=True, dtype=torch.float32)\n",
    "w0 = torch.randn(1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam([V, b, w, w0], lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward propagation\n",
    "    y_pred = forward_propagation(V, b, w, w0, train_x_apple)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mean_squared_error(y_pred, train_y_apple)\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every 1000 iterations\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Optimization finished\")\n",
    "optimal_V = V\n",
    "optimal_b = b\n",
    "optimal_w = w\n",
    "optimal_w0 = w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to see the relatiobship between different number of neurals: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_function(V, b, w, w0):\n",
    "    \"\"\"\n",
    "    Returns a prediction function using the learned parameters.\n",
    "\n",
    "    Parameters:\n",
    "    V (torch.Tensor): Learned parameter V.\n",
    "    b (torch.Tensor): Learned parameter b.\n",
    "    w (torch.Tensor): Learned parameter w.\n",
    "    w0 (torch.Tensor): Learned parameter w0.\n",
    "\n",
    "    Returns:\n",
    "    function: A function that takes input data x and returns predictions.\n",
    "    \"\"\"\n",
    "    def predict(x):\n",
    "        \"\"\"\n",
    "        Predict using the learned parameters.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input data of shape (n_samples, 2).\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The predicted values.\n",
    "        \"\"\"\n",
    "        return forward_propagation(V,b,w,w0,x)\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = get_predict_function(optimal_V, optimal_b, optimal_w, optimal_w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = final_model(test_x_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_predict, test_y_apple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将tensor数据转换为numpy数组\n",
    "test_x_apple_np = test_x_apple.detach().numpy()\n",
    "test_y_apple_np = test_y_apple.detach().numpy()\n",
    "y_predict_np = y_predict.detach().numpy()\n",
    "# 创建三维图像\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 绘制实际值\n",
    "ax.scatter(test_x_apple_np[0], test_x_apple_np[1], test_y_apple_np, c='blue', marker='o', label='Actual Values')\n",
    "\n",
    "# 绘制预测值\n",
    "ax.scatter(test_x_apple_np[0], test_x_apple_np[1], y_predict_np, c='red', marker='x', label='Predicted Values')\n",
    "\n",
    "# 设置轴标签\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "\n",
    "# 添加图例\n",
    "ax.legend()\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_function_plot(V, b, w, w0):\n",
    "    \"\"\"\n",
    "    Returns a prediction function using the learned parameters.\n",
    "\n",
    "    Parameters:\n",
    "    V (torch.Tensor): Learned parameter V.\n",
    "    b (torch.Tensor): Learned parameter b.\n",
    "    w (torch.Tensor): Learned parameter w.\n",
    "    w0 (torch.Tensor): Learned parameter w0.\n",
    "\n",
    "    Returns:\n",
    "    function: A function that takes input data x and returns predictions.\n",
    "    \"\"\"\n",
    "    def predict(K,T):\n",
    "        \"\"\"\n",
    "        Predict using the learned parameters.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input data of shape (n_samples, 2).\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The predicted values.\n",
    "        \"\"\"\n",
    "        tensor_a = torch.tensor([K])\n",
    "        tensor_b = torch.tensor([T])\n",
    "\n",
    "         # 将它们变成列向量\n",
    "        tensor_a = tensor_a.unsqueeze(1)  # 变成 1x1 矩阵\n",
    "        tensor_b = tensor_b.unsqueeze(1)  # 变成 1x1 矩阵\n",
    "        x = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "        return forward_propagation(V,b,w,w0,x).item()\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_plot = get_predict_function_plot(optimal_V, optimal_b, optimal_w, optimal_w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "\n",
    "# 生成 x 和 y 数据，并转换为 PyTorch 的 tensor\n",
    "x = torch.linspace(0, -350, 100)\n",
    "y = torch.linspace(500, 600, 100)\n",
    "\n",
    "# 创建存储 z 数据的 tensor\n",
    "z = torch.zeros((100, 100))\n",
    "\n",
    "# 使用 for 循环计算每个 (x, y) 对应的 z 值\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        z[i, j] = final_model_plot(x[i], y[j])\n",
    "\n",
    "# 将 PyTorch 的 tensor 转换为 NumPy 数组以便绘图\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "z = z.numpy()\n",
    "\n",
    "# 创建网格\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# 创建图形对象\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 绘制 3D 曲面图\n",
    "ax.plot_surface(y, x, z, cmap='viridis')\n",
    "\n",
    "# 设置标签\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3p8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
